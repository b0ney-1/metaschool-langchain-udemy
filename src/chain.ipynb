{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Understanding Memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "def create_basic_chain():\n",
        "    chatModel = ChatOpenAI(temperature=0.7, verbose=True)\n",
        "    def processMessage(userQuestion : str):\n",
        "        sysMessage = SystemMessage(content='''\n",
        "        You are a helpful assistant! Your name is Bob.\n",
        "        ''')\n",
        "    \n",
        "        humanMessage = HumanMessage(content=userQuestion)\n",
        "        return chatModel.invoke([sysMessage, humanMessage])\n",
        "    return processMessage\n",
        "\n",
        "def main():\n",
        "    qa_chain = create_basic_chain()\n",
        "    print(\"Welcome! I'm Bob, your AI assistant.\")\n",
        "    print(\"Type 'quit' to exit\\n\")\n",
        "    \n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \")\n",
        "        if user_input.lower() == 'quit':\n",
        "            break\n",
        "            \n",
        "        try:\n",
        "            response = qa_chain(user_input)\n",
        "            print(\"\\nBob:\", response.content)\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "        print(\"Please set your OPENAI_API_KEY environment variable\")\n",
        "    else:\n",
        "        main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Types of Memory\n",
        "1. Short term Memory \n",
        "2. Long term Memory "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Adding Short-Term Memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "def create_chain_with_memory():\n",
        "    chatModel = ChatOpenAI(temperature=0.7, verbose=True)\n",
        "    memory = ConversationBufferMemory(\n",
        "        return_messages=True,  # Returns messages in a structured format\n",
        "        memory_key=\"chat_history\"  # Where to store the memory\n",
        "    )\n",
        "    \n",
        "    def processMessage(userQuestion: str):\n",
        "        # Get previous chat history\n",
        "        chat_history = memory.chat_memory.messages\n",
        "        \n",
        "        # Create system message\n",
        "        sysMessage = SystemMessage(content='''\n",
        "        You are a helpful assistant! Your name is Bob.\n",
        "        ''')\n",
        "        \n",
        "        # Add new human message\n",
        "        humanMessage = HumanMessage(content=userQuestion)\n",
        "        \n",
        "        # Combine all messages\n",
        "        messages = [sysMessage] + chat_history + [humanMessage]\n",
        "        \n",
        "        # Get response\n",
        "        response = chatModel.invoke(messages)\n",
        "        \n",
        "        # Save the interaction to memory\n",
        "        memory.chat_memory.add_user_message(userQuestion)\n",
        "        memory.chat_memory.add_ai_message(response.content)\n",
        "        \n",
        "        return response\n",
        "    \n",
        "    return processMessage\n",
        "\n",
        "chain = create_chain_with_memory()\n",
        "\n",
        "response1 = chain(\"Hi Bob, my name is Starship commander\")\n",
        "print(response1.content)\n",
        "\n",
        "response2 = chain(\"What's my name?\")\n",
        "print(response2.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Managing Long Conversation History\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_chain_with_managed_memory():\n",
        "    chatModel = ChatOpenAI(temperature=0.7, verbose=True)\n",
        "    \n",
        "    # Initialize memory with a limit\n",
        "    memory = ConversationBufferMemory(\n",
        "        return_messages=True,\n",
        "        memory_key=\"chat_history\",\n",
        "        k=5  # Keep only last 5 interactions\n",
        "    )\n",
        "    \n",
        "    def processMessage(userQuestion: str):\n",
        "        # Get managed chat history\n",
        "        chat_history = memory.chat_memory.messages[-5:]  # Only last 5 messages\n",
        "        \n",
        "        sysMessage = SystemMessage(content='''\n",
        "        You are a helpful assistant! Your name is Bob. \n",
        "        ''')\n",
        "        \n",
        "        humanMessage = HumanMessage(content=userQuestion)\n",
        "        \n",
        "        # Combine all messages\n",
        "        messages = [sysMessage] + chat_history + [humanMessage]\n",
        "        \n",
        "        # Get response\n",
        "        response = chatModel.invoke(messages)\n",
        "        \n",
        "        # Save to memory\n",
        "        memory.chat_memory.add_user_message(userQuestion)\n",
        "        memory.chat_memory.add_ai_message(response.content)\n",
        "        \n",
        "        # Log memory state\n",
        "        print(\"\\nCurrent Memory State:\")\n",
        "        print(f\"Number of messages in memory: {len(memory.chat_memory.messages)}\")\n",
        "        \n",
        "        return response\n",
        "\n",
        "    return processMessage\n",
        "\n",
        "def main():\n",
        "    chain = create_chain_with_managed_memory()\n",
        "    \n",
        "    print(\"Welcome to the Memory-Managed Chat System!\")\n",
        "    print(\"Type 'quit' to exit\\n\")\n",
        "    \n",
        "    while True:\n",
        "        user_question = input(\"\\nYou: \")\n",
        "        \n",
        "        if user_question.lower() == 'quit':\n",
        "            break\n",
        "            \n",
        "        try:\n",
        "            response = chain(user_question)\n",
        "            print(\"\\nBob:\", response.content)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"\\nAn error occurred: {str(e)}\")\n",
        "            print(\"Please make sure your OpenAI API key is set correctly.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "        print(\"Please set your OPENAI_API_KEY environment variable\")\n",
        "    else:\n",
        "        main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Using Summary Memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "def create_chain_with_summary_memory():\n",
        "    chatModel = ChatOpenAI(temperature=0.7, verbose=True)\n",
        "    \n",
        "    # Initialize summary memory\n",
        "    memory = ConversationSummaryMemory(\n",
        "        llm=chatModel,\n",
        "        return_messages=True,\n",
        "        memory_key=\"chat_history\"\n",
        "    )\n",
        "    \n",
        "    def processMessage(userQuestion: str):\n",
        "        # Get the current chat history\n",
        "        chat_history = memory.chat_memory.messages\n",
        "        \n",
        "        sysMessage = SystemMessage(content='''\n",
        "        You are a helpful assistant! Your name is Bob. \n",
        "        ''')\n",
        "        \n",
        "        humanMessage = HumanMessage(content=userQuestion)\n",
        "        messages = [sysMessage] + chat_history + [humanMessage]\n",
        "        \n",
        "        # Get response\n",
        "        response = chatModel.invoke(messages)\n",
        "        \n",
        "        # Add to memory\n",
        "        memory.chat_memory.add_user_message(userQuestion)\n",
        "        memory.chat_memory.add_ai_message(response.content)\n",
        "        \n",
        "        # Print current memory state\n",
        "        print(\"\\nCurrent Memory State:\")\n",
        "        print(f\"Number of messages: {len(memory.chat_memory.messages)}\")\n",
        "        \n",
        "        # If conversation is long, show the current summary\n",
        "        if len(memory.chat_memory.messages) > 4:\n",
        "            print(\"\\nCurrent Conversation Summary:\")\n",
        "            print(memory.predict_new_summary(\n",
        "                messages=memory.chat_memory.messages,\n",
        "                existing_summary=\"\"\n",
        "            ))\n",
        "        \n",
        "        return response\n",
        "\n",
        "    return processMessage\n",
        "\n",
        "def main():\n",
        "    chain = create_chain_with_summary_memory()\n",
        "    \n",
        "    print(\"Welcome to the Summary Memory Chat System!\")\n",
        "    print(\"Type 'quit' to exit\\n\")\n",
        "    print(\"Note: After 4 interactions, you'll see a summary of the conversation\\n\")\n",
        "    \n",
        "    while True:\n",
        "        user_question = input(\"\\nYou: \")\n",
        "        \n",
        "        if user_question.lower() == 'quit':\n",
        "            break\n",
        "            \n",
        "        try:\n",
        "            response = chain(user_question)\n",
        "            print(\"\\nBob:\", response.content)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"\\nAn error occurred: {str(e)}\")\n",
        "            print(\"Please make sure your OpenAI API key is set correctly.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "        print(\"Please set your OPENAI_API_KEY environment variable\")\n",
        "    else:\n",
        "        main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
