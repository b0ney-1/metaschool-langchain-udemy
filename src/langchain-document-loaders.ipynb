{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Loaders and Text Splitters in LangChain\n",
    "\n",
    "In our previous lessons, we explored multimodal capabilities with images. Now, let's dive into handling different types of documents using LangChain. We'll learn about document loaders and how to effectively process document data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Document Loaders\n",
    "\n",
    "LangChain provides two main categories of document loaders:\n",
    "1. **File Loaders**: For loading local files (CSV, PDF, TXT, etc.)\n",
    "2. **Web Loaders**: For loading content from web sources\n",
    "\n",
    "Let's start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from langchain_community.document_loaders import CSVLoader, WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Text Splitting\n",
    "\n",
    "Before we dive into document loading, let's understand text splitting. Text splitters are crucial for processing large documents as they help break down text into manageable chunks.\n",
    "\n",
    "LangChain provides several text splitters:\n",
    "- RecursiveCharacterTextSplitter (most commonly used)\n",
    "- CharacterTextSplitter\n",
    "- TokenTextSplitter\n",
    "\n",
    "We'll use RecursiveCharacterTextSplitter as it's the most versatile option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # Number of characters per chunk\n",
    "    chunk_overlap=50,  # Number of overlapping characters\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "print(\"Text splitter initialized with chunk size of 500 and overlap of 50 characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Loader Example: CSV\n",
    "\n",
    "Let's start with loading a CSV file. We'll use a sample customer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_and_process_csv():\n",
    "    # Initialize the CSV loader\n",
    "    loader = CSVLoader(\n",
    "        file_path=\"customers-100.csv\",\n",
    "        csv_args={\n",
    "            'delimiter': ',',\n",
    "            'quotechar': '\"',\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Load the documents\n",
    "    documents = loader.load()\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "    print(f\"Created {len(splits)} splits\")\n",
    "    \n",
    "    # Take first few chunks to stay within token limits\n",
    "    limited_splits = splits[:5]\n",
    "    \n",
    "    return \"\\n\\n\".join([doc.page_content for doc in limited_splits])\n",
    "\n",
    "context = load_and_process_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our processed CSV data, let's set up a simple QA chain to query it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def setup_qa_chain():\n",
    "    # Initialize OpenAI model\n",
    "    llm = OpenAI(temperature=0)\n",
    "    \n",
    "    # Create prompt template\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"Based on the following customer data, please answer the question.\n",
    "        \n",
    "        Customer Data:\n",
    "        {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Answer: \"\"\",\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    \n",
    "    return prompt | llm\n",
    "\n",
    "# Set up the chain\n",
    "qa_chain = setup_qa_chain()\n",
    "\n",
    "# Test with a sample question\n",
    "question = \"What is sheryl's email address?\"\n",
    "response = qa_chain.invoke({\"context\": context, \"question\": question})\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Loader Example\n",
    "\n",
    "Now let's look at how we can load content from web pages. We'll use the WebBaseLoader to fetch and process web content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_and_process_webpage(url):\n",
    "    # Initialize web loader with custom headers\n",
    "    loader = WebBaseLoader(\n",
    "        url,\n",
    "        verify_ssl=False,\n",
    "        header_template={\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Load and process the webpage\n",
    "    documents = loader.load()\n",
    "    print(f\"Loaded webpage with {len(documents)} documents\")\n",
    "    \n",
    "    # Split the content\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "    print(f\"Created {len(splits)} splits\")\n",
    "    \n",
    "    limited_splits = splits[:3]\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in limited_splits])\n",
    "\n",
    "# Test with a sample URL\n",
    "url = \"https://en.wikipedia.org/wiki/LangChain\"\n",
    "web_context = load_and_process_webpage(url)\n",
    "\n",
    "# Test with some questions\n",
    "questions = [\n",
    "    \"What is LangChain?\",\n",
    "    \"What are the main features of LangChain?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    response = qa_chain.invoke({\"context\": web_context, \"question\": question})\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"Answer: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. Document loaders help us process different types of documents (files and web content)\n",
    "2. Text splitting is crucial for handling large documents effectively\n",
    "3. The same QA chain can be used with different types of document loaders\n",
    "4. RecursiveCharacterTextSplitter is versatile for most use cases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
